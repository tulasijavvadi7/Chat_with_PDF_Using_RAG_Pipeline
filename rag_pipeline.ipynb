{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama PDF RAG Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pdfplumber\n",
    "from PIL import Image\n",
    "import io\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set environment variable for protobuf\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF loaded successfully: Tables_Charts_Graphs.pdf\n"
     ]
    }
   ],
   "source": [
    "# Load PDF with PyPDFLoader\n",
    "local_path = \"Tables_Charts_Graphs.pdf\"\n",
    "\n",
    "def load_pdf():\n",
    "    \"\"\"Loads the PDF and extracts text\"\"\"\n",
    "    if local_path:\n",
    "        loader = PyPDFLoader(local_path)\n",
    "        data = loader.load()\n",
    "        print(f\"PDF loaded successfully: {local_path}\")\n",
    "        return data\n",
    "    else:\n",
    "        print(\"Upload a PDF file\")\n",
    "        return None\n",
    "\n",
    "data = load_pdf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 1\n",
      "Processing page 2\n",
      "Processing page 3\n",
      "Processing page 4\n",
      "Processing page 5\n",
      "Processing page 6\n",
      "Processing page 7\n",
      "Processing page 8\n",
      "Error extracting image on page 8: cannot identify image file <_io.BytesIO object at 0x000001EE9032F510>\n",
      "Processing page 9\n",
      "Error extracting image on page 9: cannot identify image file <_io.BytesIO object at 0x000001EE90792BB0>\n",
      "Error extracting image on page 9: cannot identify image file <_io.BytesIO object at 0x000001EE90792BB0>\n",
      "Error extracting image on page 9: cannot identify image file <_io.BytesIO object at 0x000001EE90792BB0>\n",
      "Error extracting image on page 9: cannot identify image file <_io.BytesIO object at 0x000001EE90792BB0>\n",
      "Error extracting image on page 9: cannot identify image file <_io.BytesIO object at 0x000001EE90792BB0>\n",
      "Error extracting image on page 9: cannot identify image file <_io.BytesIO object at 0x000001EE90792BB0>\n",
      "Error extracting image on page 9: cannot identify image file <_io.BytesIO object at 0x000001EE90792BB0>\n",
      "Error extracting image on page 9: cannot identify image file <_io.BytesIO object at 0x000001EE90792BB0>\n",
      "Error extracting image on page 9: cannot identify image file <_io.BytesIO object at 0x000001EE90792BB0>\n",
      "Error extracting image on page 9: cannot identify image file <_io.BytesIO object at 0x000001EE90792BB0>\n",
      "Error extracting image on page 9: cannot identify image file <_io.BytesIO object at 0x000001EE90792BB0>\n",
      "Error extracting image on page 9: cannot identify image file <_io.BytesIO object at 0x000001EE90792BB0>\n",
      "Error extracting image on page 9: cannot identify image file <_io.BytesIO object at 0x000001EE90792BB0>\n",
      "Processing page 10\n",
      "Processing page 11\n",
      "Processing page 12\n",
      "Processing page 13\n",
      "Processing page 14\n",
      "Processing page 15\n",
      "Processing page 16\n",
      "Error extracting image on page 16: cannot identify image file <_io.BytesIO object at 0x000001EE90BE7E70>\n",
      "Processing page 17\n",
      "Error extracting image on page 17: cannot identify image file <_io.BytesIO object at 0x000001EE90B87E70>\n",
      "Processing page 18\n",
      "Error extracting image on page 18: cannot identify image file <_io.BytesIO object at 0x000001EE90C2C9F0>\n",
      "Processing page 19\n",
      "Error extracting image on page 19: cannot identify image file <_io.BytesIO object at 0x000001EE8FEFA6B0>\n",
      "Error extracting image on page 19: cannot identify image file <_io.BytesIO object at 0x000001EE8FEFA6B0>\n",
      "Extracted 2 tables and 1 images\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Extract Tables and Images with pdfplumber\n",
    "def extract_tables_and_images(pdf_path):\n",
    "    tables = []\n",
    "    images = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            print(f\"Processing page {page_num + 1}\")\n",
    "            \n",
    "            # Extract tables\n",
    "            table = page.extract_table()\n",
    "            if table:\n",
    "                tables.append((page_num + 1, table))  # Store page number with table\n",
    "            \n",
    "            # Extract images (graphs, charts)\n",
    "            for img in page.images:\n",
    "                try:\n",
    "                    # Check if the image stream is valid before processing\n",
    "                    img_stream = img.get('stream')\n",
    "                    if img_stream:\n",
    "                        img_bytes = img_stream.get_data()  # Extract raw image data\n",
    "                        if img_bytes:\n",
    "                            try:\n",
    "                                im = Image.open(io.BytesIO(img_bytes))  # Load image from raw bytes\n",
    "                                im.verify()  # Verify if the image is valid\n",
    "                                im = Image.open(io.BytesIO(img_bytes))  # Re-open image after verification\n",
    "                                images.append((page_num + 1, im))  # Store page number with image\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error extracting image on page {page_num + 1}: {e}\")\n",
    "                    else:\n",
    "                        print(f\"Skipping non-image content on page {page_num + 1}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image on page {page_num + 1}: {e}\")\n",
    "    \n",
    "    return tables, images\n",
    "\n",
    "# Extract tables and images\n",
    "local_path = \"tables_Charts_Graphs.pdf\"\n",
    "tables, images = extract_tables_and_images(local_path)\n",
    "print(f\"Extracted {len(tables)} tables and {len(images)} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 18 chunks\n"
     ]
    }
   ],
   "source": [
    "# Split text into chunks\n",
    "def split_text(data):\n",
    "    \"\"\"Splits the extracted text into smaller chunks\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    print(f\"Text split into {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "chunks = split_text(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create vector database\n",
    "def create_vector_db(chunks):\n",
    "    \"\"\"Creates a vector database from the document chunks\"\"\"\n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "        collection_name=\"local-rag\"\n",
    "    )\n",
    "    print(\"Vector database created successfully\")\n",
    "    return vector_db\n",
    "\n",
    "vector_db = create_vector_db(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up LLM and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LLM and retrieval\n",
    "local_model = \"llama3.2\"  # Or whichever model you prefer\n",
    "llm = ChatOllama(model=local_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Query prompt template for question generation\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate 2\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "# Set up retriever\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAG prompt template for answering questions\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"# RAG prompt template for answering questions\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with PDF - Enhance with Tables and Images\n",
    "def chat_with_pdf(question):\n",
    "    \"\"\"\n",
    "    Chat with the PDF using the RAG chain and also handle tables and images.\n",
    "    \"\"\"\n",
    "    # If the question asks for tables\n",
    "    if \"table\" in question.lower():\n",
    "        if tables:\n",
    "            print(\"Table data from the document:\")\n",
    "            for table in tables:\n",
    "                for row in table:\n",
    "                    print(row)  # Print table row-by-row\n",
    "        else:\n",
    "            print(\"No tables found in the document.\")\n",
    "    \n",
    "    # If the question asks for graphs (images/charts)\n",
    "    elif \"graph\" in question.lower() or \"chart\" in question.lower():\n",
    "        if images:\n",
    "            for img in images:\n",
    "                img.show()  # Show the image using PIL's show() method\n",
    "        else:\n",
    "            print(\"No images (graphs/charts) found in the document.\")\n",
    "    \n",
    "    else:\n",
    "        # For regular text-based queries, use the RAG chain\n",
    "        response = chain.invoke(question)\n",
    "        print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The main idea of this document appears to be an introduction to visual representations of data, specifically tables, charts, and graphs. The document provides examples from various fields such as history, economics, education, psychology, urban affairs, and everyday life, with a focus on explaining when to use different types of graphs (line graph, pie chart, bar graph) to visualize data.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example Questions\n",
    "chat_with_pdf(\"What is the main idea of this document?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: There is no information about unemployment details for master's degree holders in the provided context. The text only discusses various types of graphs and charts, provides examples from history, economics, education, psychology, urban affairs, and everyday life, but does not mention unemployment or specific data related to master's degree holders.\n"
     ]
    }
   ],
   "source": [
    "chat_with_pdf(\"Extract the unemployment details for master's degree holders from page 2 of the document.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: No, I couldn't find any unemployment information based on degree type in the provided context. The text only discusses examples and tables related to GDP, Education, Psychology, Urban Affairs, and Everyday Life, but does not mention unemployment rates or any specific data regarding degree types.\n"
     ]
    }
   ],
   "source": [
    "chat_with_pdf(\"Can you provide the unemployment information based on degree type from page 2?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table data from the document:\n",
      "['Year', '2010', '2011', '2012', '2013', '2014', '2015']\n",
      "['All Industries', '26093515', '27535971', '28663246', '29601191', '30895407', '31397023']\n",
      "['Manufacturing', '4992521', '5581942', '5841608', '5953299', '6047477', '5829554']\n",
      "['Finance,\\nInsurance, Real\\nEstate, Rental,\\nLeasing', '4522451', '4618678', '4797313', '5031881', '5339678', '5597018']\n",
      "['Arts,\\nEntertainment,\\nRecreation,\\nAccommodation,\\nand Food Service', '964032', '1015238', '1076249', '1120496', '1189646', '1283813']\n",
      "['Other', '15614511', '16320113', '16948076', '17495515', '18318606', '18686638']\n",
      "['', '', '', '', None, None, None]\n",
      "['', '', '', None, None, None, None]\n",
      "['', '', None, None, None, None, None]\n",
      "['', None, None, None, None, None, None]\n",
      "['', '', None, None, None, None, None]\n",
      "['', None, None, None, None, None, None]\n",
      "['', '', '', '', '', '', '']\n",
      "['', '', '', '', '', '', None]\n"
     ]
    }
   ],
   "source": [
    "chat_with_pdf(\"Extract the table data from page 6.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main idea of this document appears to be an introduction or guide on using charts and graphs to visualize data. The document covers various types of visual representations (tables, line graphs, pie charts, bar graphs), when to use each one, and provides examples from different fields such as economics, education, psychology, and urban affairs.\n"
     ]
    }
   ],
   "source": [
    "# Example Questions\n",
    "chat_with_pdf(\"What is the main idea of this document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: There is no mention of \"unemployment\" or a specific page number related to \"degree input\" in the provided context. The documents only contain information about tables, charts, and graphs with examples from various fields such as history, economics, education, psychology, urban affairs, and everyday life. There is no relevant content regarding unemployment or degree input on any of the pages mentioned.\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "chat_with_pdf(\"Extract the unemployment details from page 2 of the document on type of degree input.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no clear \"case study\" highlighted in the document. The document appears to be a guide or tutorial on using tables, charts, and graphs to visualize data, with examples from various fields such as education, urban affairs, psychology, economics, and history. It provides explanations, guidelines, and illustrations on when to use different types of charts (line graph, pie chart, bar graph) and how to plot data. However, it does not present a specific case study or scenario that requires explanation.\n"
     ]
    }
   ],
   "source": [
    "# Example 3\n",
    "chat_with_pdf(\"Can you explain the case study highlighted in the document?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database deleted successfully\n"
     ]
    }
   ],
   "source": [
    "# Optional: Clean up when done \n",
    "vector_db.delete_collection()\n",
    "print(\"Vector database deleted successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
